{"prompt": "Text:\n\nAll right. And, and as a reminder, you're welcome to, uh, ask, uh, questions, um, on the chat. Um, monitoring it as we go. So friendly reminders, um, homework 10, the neural machine translation, uh, is due tonight. Um, uh, and, uh, homework, uh, sorry. And milestone two is also do due tonight if you're doing that, uh, if you're doing the term project option, um, and then, uh, we just looked at the perspectives project that uses Burt, uh, and, uh, will assign your project mentors on Friday. Um, and then apologize. This is out of date.\n\nSummary:", "completion": "Homework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\n\nText:\n\nThe quiz, uh, will be due on Monday and it's gonna cover the next batch of, uh, batch of readings. So today we're gonna talk about, uh, a super cool, um, topic. That's I think one of the most fascinating, um, industrial, uh, applications of, um, natural language processing to happen in the past few years. So today we're gonna cover chapter 26 of the GRE ski and Martin textbook look, which looks at dialogue systems and chat bots.\n\nSummary:", "completion": "The quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\n\nText:\n\nSo, um, dialogue systems are things like Amazon Alexa, which can, uh, execute commands based on things that you say to it. So you can tell it things like, uh, you know, um, play some music or you can have it, uh, um, turn on or off your lights or schedule a calendar appointment, uh, or do, uh, other kind of home automation things. Um, Uh, you can have it, uh, answer questions so you can ask it, uh, um, uh, useful things like, uh, what's the weather gonna be like today, or, uh, you can ask it.\n\nSummary:", "completion": "Dialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\n\nText:\n\nSo my kids they're totally into our Amazon Alexa. Um, uh, the funniest question I heard them ask recently was, um, do turtles breathe through their butts  and it turns out they do. I don't know where my kids learned that, but, uh, at least Alexa claims that it's true. Um, uh, so the others, like more abstractly than just, uh, Amazon Alexa or, uh, Google assistant or, um, uh, Siri on your apple devices are this idea that you need some like, or that there's a desire to have a human, um, more hu more humans.\n\nSummary:", "completion": "The funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\n\nText:\n\nMore human language oriented interface to smart devices or to robots or things like that.\n\nSummary:", "completion": "There is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\n\nText:\n\nSo, uh, one of the earliest versions of this is from the movie 2001, a space Odyssey, which is an epic sci-fi movie from the mid 1970s, where they had this, um, really prescient set of technologies, including, um, a voice interface for their spaceship called how 9,000, where, how is just, uh, the reason that they picked the name, how is because it's, um, IBM, uh, offset by one  so, uh, you add one to the, um, I, and you get, uh, uh, or sorry, subtract one.\n\nSummary:", "completion": "In the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\n\nText:\n\nAnd you get, um, uh, H uh, you subtract one from B and you get. you subtract one from M and you get L so it was a vision of like what computers ought to be like at a time that was like way before there were even, um, common like household computers. Uh, so before the era of personal computers, their vision, um, science fiction writers envisioned, uh, this technology that would really interact with people, um, via voice and via, via natural language. Um, so the textbook chapter, uh, covers two different, uh, types of dialogue systems.\n\nSummary:", "completion": "The vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\n\nText:\n\nUm, uh, the first one that we're gonna take a look at are called chat bots. So these are, uh, mainly, um, exercises to see if we could imitate, uh, the style of human conversations. So could we have. An intelligent agent that actually is capable of carrying on a conversation with the person. Um, and I'm academically interested in this, but I think practically I'm a little dubious about like it's usefulness. Uh, I think it's really kind of for fun and entertainment.\n\nSummary:", "completion": "The first type of dialogue system are called chat bots, which are mainly used for entertainment."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\n\nText:\n\nSo I can see applications where if you built a really compelling dialogue, uh, sorry, a really compelling chat bot style system, then you could imagine that it would be a useful thing for things like video games, where you have, um, an artificial intelligence controlling other players that you interact with and have conversations with. Um, uh, you know, and it's kind of like cool in sci-fi. So it's like if you've seen the movie, her, uh, where Joaquin Phoenix ends up falling in love with his computer operating systems,  because it's this kind of chat lock.\n\nSummary:", "completion": "There are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\n\nText:\n\nUm, then that's like a vision of what these chatbot. But I, I think the more, uh, currently interesting and currently useful, uh, type of dialogue system are task oriented dialogue systems. So these are things where you're actually trying to get something done and it's simply faster to do it with your voice, um, than it is by typing things in or looking things up on the internet. So this are things like, you know, trying to make a calendar appointment with Siri, um, which was, uh, old school dialogue system that was developed as part of DARPA.\n\nSummary:", "completion": "The more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri."}
{"prompt": "Previous summaries:\n\nHomework 10 is due tonight, and milestone two is also do due tonight if you are doing the term project option. Apologies for being out of date.\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\n\nText:\n\nSo that, uh, military commanders could ask questions about the position of, uh, of their fleets in the oceans and to do command and control type things. Um, so, uh, let's think about first, like why human. Conversations are actually like interesting from a linguistic perspective and what sorts of things, uh, make them challenging. Um, so the textbook, uh, goes into, uh, details about different elements that linguists have studied, uh, over time, um, uh, of the process of people having conversations, um, that make them interesting. So I'm gonna take a look into a few of these.\n\nSummary:", "completion": "Human conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing."}
{"prompt": "Previous summaries:\n\nThe quiz will be due on Monday and it will cover the next batch of readings. Today we will discuss dialogue systems and chat bots, which are two cool applications of natural language processing.\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\n\nText:\n\nSo I'm gonna talk about like, why taking turns matters and why it's challenging for computers. I'm gonna introduce you to a notion called speech acts. So it's like the notion that you can, um, model, or you can think about conversational turns as having some sort of, um, intense, and you can break those down to like subcategories. Um, one thing I'm not gonna talk about is the notion of grounding, where there's really interesting psycho linguistic studies.\n\nSummary:", "completion": "Taking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other."}
{"prompt": "Previous summaries:\n\nDialogue systems are applications that can execute commands based on things that are said to them. For example, one could ask Alexa for the weather forecast or how many calories are in a slice of pizza.\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\n\nText:\n\nYou can see that people end up like accommodating each other and having some shared understanding, um, uh, that really shapes how you refer to things. And, uh, this understanding of like what other people's understand other people understand, like really helps facilitate conversations. So there's like super interesting theory of mind stuff going on there. Um, and, uh, we can even think about dialogue structure in a similar way that we thought about sentence structure. So you can expand that notion out into much larger, um, uh, units.\n\nSummary:", "completion": "The understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure."}
{"prompt": "Previous summaries:\n\nThe funniest question my kids have asked Alexa is whether turtles breathe through their butts. Abstractly, there is a desire to have a human level of understanding for natural language.\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\n\nText:\n\nSo you can think about like, um, what the larger structure is when you're contributing at the next sentence. Like, is it elaborating on something? Is it contradicting it? Um, that sort of thing. Um, so that's a larger discourse structure. Um, we'll also talk about, uh, implicature in a, in a couple of slides. okay. So the first one is a pretty obvious thing, but it ends up, uh, having significant implications if you're trying to build a dialogue system.\n\nSummary:", "completion": "The first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation."}
{"prompt": "Previous summaries:\n\nThere is a desire for a human level of understanding for natural language, which would allow for a more human language oriented interface to smart devices.\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\n\nText:\n\nSo, um, the fact is that, uh, when we're having a conversation with each other, we're basically taking turns in the same way that we would in a game. Right? Like, it's your move, it's my move. Um, and so each turn, uh, we defer to the other person, um, at the end of our turn to try to listen to what they're gonna say next. Um, and it could be really disconcerting if people don't, uh, like produce the pause that you need to like clue you in that you're allowed to speak now.\n\nSummary:", "completion": "When we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak."}
{"prompt": "Previous summaries:\n\nIn the movie 2001: A Space Odyssey, the characters interact with a voice interface for their spaceship called how 9,000. How is an acronym for IBM, which is offset by one.\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\n\nText:\n\nUm, so, uh, being able to know, uh, when is an appropriate place to start speaking and when, uh, Uh, and similarly, like when someone has stopped is a super important thing for having a natural flow to a conversation, uh, by a computer. So, uh, this is more relevant for spoken dialogue systems than for ones that are like text based because they're, you get a nice signal or someone sends you a message and that's the end of the message.\n\nSummary:", "completion": "Being able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones."}
{"prompt": "Previous summaries:\n\nThe vision of computers in science fiction is one of technology that interacts with people via voice and natural language. The textbook chapter covers two different types of dialogue systems: interactive and through world.\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\n\nText:\n\nBut for spoken dialogue systems, you have to actively, um, listen and actively wait until you detect the end of, uh, the, uh, speaker's turn. And there can often be pauses while someone's collecting their thoughts or other, uh, reasons that could be a signal. Um, it could be a false signal that they've reached the end of their turn. So there's a notion of like end point detection where a system has to figure out like for a. Period of time when they detect no speech activity, is that the end of the speaker's turn or not.\n\nSummary:", "completion": "For spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true."}
{"prompt": "Previous summaries:\n\nThe first type of dialogue system are called chat bots, which are mainly used for entertainment.\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\n\nText:\n\nUm, and so if it's not, then the system might receive like less information than the, um, user intended, or it might like seem rude because it's gonna start generating its own response in a way that interrupts the user. Um, so that's the notion of turn taking and why it's relevant. So that's a pretty simple notion. Um, there's a linguistic theory of speech acts, which, uh, thinks of every utterance in a dialogue as some kind of action being performed by the speaker.\n\nSummary:", "completion": "The notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker."}
{"prompt": "Previous summaries:\n\nThere are applications for chat bots in video games and movies. For example, in the movie Her, the main character falls in love with his computer operating system because it is a very sophisticated chat bot.\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\n\nText:\n\nSo these actions, these speech acts are, um, are, uh, either called speech acts or sometimes dialogue acts. Uh, so if you're. asking a person or a dialogue, something, a dialogue system to do something like if you're saying to Siri, turn up the music, then that's, uh, issuing a directive, right. And sometimes questions can be indirect directive. So you can say, uh, um, um, a way of commanding the system to produce an answer.\n\nSummary:", "completion": "Speech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect."}
{"prompt": "Previous summaries:\n\nThe more interesting and useful type of dialogue system are task oriented dialogue systems, which are used for getting things done. For example, trying to make a calendar appointment with Siri.\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\n\nText:\n\nUm, so directives are probably the most relevant for, um, things like, uh, the dialogue systems, the task based dialogue systems that we'll talk about in the second half of this lecture. Um, but there's other, uh, super useful things here, um, where, uh, the linguistic theory can help us build better systems. There's things like, uh, S which are, uh, saying that. asserting something to be the case. Right. So I'm stating something will be the case.\n\nSummary:", "completion": "Directives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case."}
{"prompt": "Previous summaries:\n\nHuman conversation is interesting from a linguistic perspective because it involves the use of metaphors and entailing.\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\n\nText:\n\nI could say something, um, uh, like, um, I need to book a plane ticket, um, for a conference that will happen in may, then that adds some kind of constraint to the dialogue system, um, without directly issuing a command. Um, uh, and then there's other really interesting theories of different types of these things like coves, where you're committing the speaker to some future course of action, where you're promising to do something.\n\nSummary:", "completion": "There are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action."}
{"prompt": "Previous summaries:\n\nTaking turns is important for human conversation because it allows us to engage in understanding. Speech acts are a way of understanding conversational turns. There is also the notion of grounding, which is where there is interesting research on how humans understand each other.\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\n\nText:\n\nUm, and that acknowledgements are a super important part of, um, uh, conversations between humans, uh, that you can potentially replicate in machines for a more interesting, useful, uh, uh, dialogue system. okay. So the third linguistic thing that I'm gonna mention here and that, and that cover the textbook covers in more detail is a notion of conversational implicature. So if I build a, um, travel agent, uh, and, uh, it asks some question, like what day in may do you want to travel in, um, uh, the response that we get might not directly answer the question.\n\nSummary:", "completion": "Acknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question."}
{"prompt": "Previous summaries:\n\nThe understanding of what other people understand helps facilitate conversations. This understanding can be applied to dialogue structure in a similar way that it has been applied to sentence structure.\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\n\nText:\n\nSo if the person using my system says, I need to be there for a meeting that's, uh, from the 12th to the 15th, then, uh, it's not saying like what day they want to travel. It's giving me information. That's relevant to this question. And I'm able to infer that, um, even though you could pair other like totally random conversational, uh,  things here that would be irrelevant, but still kind of as relevant as this actually truly relevant one is. Right.\n\nSummary:", "completion": "The system is able to infer the user's desired date based on the information that is relevant to the question."}
{"prompt": "Previous summaries:\n\nThe first type of structure that is important for dialogue systems are discourse structures, which describe the relationship between sentences in a conversation.\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\n\nText:\n\nSo I need to make some kind of, uh, inference and in particular, I need as a listener to understand that the speaker is actually playing a long cooperatively. They're actually giving me information that's relevant, even though they're not directly answering my question. So, uh, that kind of inference that I'm drawing is a very specific kind of inference called the conversational implicature.\n\nSummary:", "completion": "The system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature."}
{"prompt": "Previous summaries:\n\nWhen we are having a conversation with each other, we take turns in the same way that we would in a game. Each turn, we defer to the other person to indicate that it is now our turn to speak.\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\n\nText:\n\nUh, so this notion of conversational implicature, um, something that was, uh, constructed by a famous linguist, a famous, uh, linguist called grace, um, who, uh, had a bunch of different like, um, rules for conversations, which are called Gracie maxims. Uh, and so the one that's relevant here  is the maximum of relevance, fittingly enough. Um, and so that, uh, the idea is that, um, in order for people to have conversations that work, that we have to follow a bunch of these rules for conversations.\n\nSummary:", "completion": "The notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations."}
{"prompt": "Previous summaries:\n\nBeing able to know when it is an appropriate time to speak is important for having a natural flow to a conversation. This is more relevant for spoken dialogue systems than for text based ones.\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\n\nText:\n\nSo the, uh, maximum of relevance or, uh, of relation is that, um, the people in the conversation are gonna try to say something relevant and that's pertinent to the discussion. Um, and the other, uh, maximums are the maximums of quantity. So that means that the people involved in the conversation are gonna be as informative as possible, um, and giving as much information as needed, but not more than that. Right.\n\nSummary:", "completion": "The maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness."}
{"prompt": "Previous summaries:\n\nFor spoken dialogue systems, we have to actively listen and wait until we detect the end of the speaker's turn. There can be false signals that indicate the end of the turn, so there is a notion of end point detection where the system has to figure out whether or not those are true.\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\n\nText:\n\nSo they're not gonna go on and on and on, like in this case, the client won't, uh, it's not super relevant, uh, Uh, the, uh, uh, the maximum quantity would, uh, dictate that they tell me only enough that it's relevant for me to take the action of understanding when they want to travel. But, uh, it wouldn't be appropriate for them to tell me like what conference they're attending and what paper they're presenting and all these sorts of things, because it's not, um, it's not pertinent to this conversation.\n\nSummary:", "completion": "The maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness."}
{"prompt": "Previous summaries:\n\nThe notion of turn taking is important for spoken dialogue systems because if the system does not take a turn in a reasonable amount of time, it may seem rude. Speech act theory is a linguistic theory that thinks of every utterance as some kind of action being performed by the speaker.\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\n\nText:\n\nSo all these things that like dictate how conversations are shaped are nicely thought about by linguists. Okay. And they're really nicely dealt with it in the book. And, um, uh, I encourage you to read more about it. Okay. So now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that, um, a system can understand. So the. so, uh, we're gonna look at a couple of different arch architectures for, uh, chatbots.\n\nSummary:", "completion": "The rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand."}
{"prompt": "Previous summaries:\n\nSpeech acts are actions that are performed by the speaker through the utterance. Some directives are direct and some are indirect.\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\n\nText:\n\nSo, um, there's, uh, some architectures that were used in the past, um, uh, that were role based. So we'll take a look back at two historical systems. One of the very earliest, um, chatbots called Eliza, which, uh, was a cool, um, program that mimicked conversation by picking a very stereotypical typo conversation and, uh, trying to imitate it. Um, and then we'll shift gears and look at more modern systems, which are all data driven.\n\nSummary:", "completion": "There are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven."}
{"prompt": "Previous summaries:\n\nDirectives are the most relevant for dialogue systems, but there are other useful things that can be done with this linguistic theory. For example, S is saying that something is the case.\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\n\nText:\n\nUm, so we'll look at two different approaches to data driven chat bot systems, one that uses information retrieval to pick out relevant responses. And then one that's very similar to the architectures that we looked at for machine translation that uses a neural network en coder decode. Um, to produce relevant responses. So, uh, uh, chat bots are actually like, uh, or neural network based chat. Bots are something that like really reinvigorated this area of natural language processing research. It hadn't been like the, um, most trendy or interesting thing.\n\nSummary:", "completion": "There are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses."}
{"prompt": "Previous summaries:\n\nThere are different types of directives that can be done through this theory. Some are direct and some are indirect. There is also the notion of committing the speaker to some future course of action.\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\n\nText:\n\nA lot of super solid good research had been done on it. But in terms of like the number of paper submissions in this area, it was a pretty small subfield within NLP. Um, but within neural networks, like you suddenly had this cool tool that you could apply to certain types of data, right? So, uh, you can think about machine translation as that kind of data. And people figured out a way of, um, treating dialogue systems or chatbots in particular.\n\nSummary:", "completion": "Neural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks."}
{"prompt": "Previous summaries:\n\nAcknowledgements are a super important part of conversations between humans. This can be replicated in machines for a more interesting, useful dialogue system. Conversational implicature is the understanding that what is said may not exactly answer the question.\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\n\nText:\n\nAs, uh, a similar kind of framework, um, and that like really gave a boost in terms of the popularity of people publishing on this research topic. Okay. So before we jump into those modern architectures, let's take a look back in the past.\n\nSummary:", "completion": "The popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder."}
{"prompt": "Previous summaries:\n\nThe system is able to infer the user's desired date based on the information that is relevant to the question.\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\n\nText:\n\nUm, so the earliest historical example of, um, a dialogue system or a chat bot is one called Eliza, uh, which, uh, uh, succeeds to the extent that it does because it's imitating a particular style of conversation, um, that you would have with, uh, a therapist, like in particular, it's imitating a, um, psychologist who practiced things in Rogerian psychology. So this is a super weird kind of, uh, psychotherapy where basically the, um, psychologist, uh, sit. And as you talk through your problems and only ask questions, right?\n\nSummary:", "completion": "The earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology."}
{"prompt": "Previous summaries:\n\nThe system needs to make an inference in order to understand that the speaker is playing a cooperative game. This is called a conversational implicature.\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\n\nText:\n\nSo they're not gonna diagnose like, what's wrong with you or what action you should take, or even really make any assumptions about, um, why it is that you did something really their job is to, uh, ask you questions in order to elicit, uh, like more thoughtful reflection on your part to figure out like what the underlying cause of your anxieties is. Okay. And so, like here's a, um, example transcript from the Eliza system. So someone using this system, um, says men are all alike.\n\nSummary:", "completion": "The job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user."}
{"prompt": "Previous summaries:\n\nThe notion of conversational implicature comes from a famous linguist named Grice. The idea is that in order for people to have conversations that work, they have to follow a bunch of rules for conversations.\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\n\nText:\n\nAnd then Eliza says, in what way, they're always bugging us about something or other. And Eliza says, can you think of a specific example? And then the user says, well, my boyfriend made me come here. your boyfriend made you come here. He says I'm depressed a lot of the time. I'm so sorry to hear that you're depressed. Uh, who else in your family takes care of you? My father, your father. Uh, you're like my father in some ways. What resemblance do you see?\n\nSummary:", "completion": "The user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center."}
{"prompt": "Previous summaries:\n\nThe maximum of relevance or relation is the idea that people in a conversation will try to say something relevant and pertinent to the discussion. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\n\nText:\n\nYou're not very aggressive, but I think that you don't want me to notice that what makes you think I am not aggressive? You don't argue with me. Why do you, why do you think I don't argue with you? You're afraid of me. Uh, does it please you to believe that I'm afraid of you? My father is afraid of everybody. Um, what else comes to mind? What do you think of your father bullies, blah, blah, blah. Right? Um, so the way that this is working is, uh, through just a series of regular expressions, right?\n\nSummary:", "completion": "The user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody."}
{"prompt": "Previous summaries:\n\nThe maximum of relevance is the idea that people will try to say things that are relevant in a conversation. The maximum of quantity is the idea that people will be as informative as possible while not exceeding that level of informativeness.\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\n\nText:\n\nSo here's an example of, uh, matching role from, uh, Eliza. So it's taking a pattern with the cleany star operator. so. star once again, just matches anything. So anything before the word, you, and then, um, the parenthesis match, like the thing in the second cleany star operator up until the word me. Um, and then it'll transform that by taking that thing in the middle, between you and me and sticking it into a question, what makes you think I blank you? Right.\n\nSummary:", "completion": "The system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me."}
{"prompt": "Previous summaries:\n\nThe rules that dictate how conversations are shaped are nicely dealt with in the book. Now let's think about how do we build systems that actually try to mimic human conversations and try to process human conversations in a way that a system can understand.\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\n\nText:\n\nSo if, uh, uh, the user said something like, I think you must really hate me. It would capture that stuff between you and me must really hate, and then it would insert it into that question. What makes you think I must really hate you? Um, so that's the complete, uh, depth of understanding of Eliza. Um, and the basic idea of Eliza is it's. Trying to pick out, um, a bunch of keywords, uh, and apply these patterns. So the textbook like gives a very simple, uh, explanation of how it's working.\n\nSummary:", "completion": "The idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working."}
{"prompt": "Previous summaries:\n\nThere are some architectures that were used in the past that were role based. One of the very earliest chatbots was called Eliza, which was a program that mimicked conversation by picking a very stereotypical type of conversation and trying to imitate it. More modern systems are all data driven.\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\n\nText:\n\nUm, it's kind of interesting in that it fails. It's been storing possible responses on a stack. So if it craps out and can't apply any of its rules to transform a sentence into another sentence, it'll just back up to some previous part of the conversation. Um, and, um, output, something like that. So if you mention, uh, you know, your parents or something like that, it'll say, tell me more about your mother, um, and, uh, pop something off the stack. Um, so that's the idea of Eliza?\n\nSummary:", "completion": "The idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack."}
{"prompt": "Previous summaries:\n\nThere are two different approaches to data driven chat bot systems. The first uses information retrieval to pick out relevant responses. The second uses a neural network en coder decode to produce relevant responses.\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\n\nText:\n\nIt, it was basically like a cool parlor trick with, um, uh, regular expressions in order to, uh, seem like it's engaging in a conversation. um, uh, shortly after Eliza, there were, uh, uh, other systems that used the same idea, but got a little bit more sophisticated. Um, so one of the early ones, uh, was called Perry where, um, it added in something deeper than Eliza in that it wasn't trying to do this, like Rosa psychologist, totally neutral thing. But instead it was meant to embody, um, some kind of personality traits.\n\nSummary:", "completion": "Perry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits."}
{"prompt": "Previous summaries:\n\nNeural networks were a cool tool that could be applied to certain types of data. People figured out a way of treating dialogue systems or chatbots in particular using neural networks.\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\n\nText:\n\nUm, so it was, uh, meant to like, be this kind of, uh, you know, unemployed person who bet money at racetracks and like had a personality and embodied like anger issues and expressed fear and mistrust and, and could respond in a way that was like hostile. Right. So it was actually like trying to do cool. Um, Uh, things with like modeling mental states of a potential agent.\n\nSummary:", "completion": "Perry was meant to be an agent with personality traits. It tried to model mental states and express them."}
{"prompt": "Previous summaries:\n\nThe popularity of neural networks gave a boost to the research of dialogue systems. There are two different approaches to using neural networks for dialogue systems, one that uses information retrieval and one that uses a neural network encoder decoder.\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\n\nText:\n\nSo you could imagine that that would be, uh, really interesting if you were trying to design like interactive fiction or have characters, uh, in a game that you could play with, you would really want them to, uh, take on a persona, um, and act appropriate for that persona. And in fact, some, um, of the interesting recent work in neural network based, um, chat bots, uh, has collected a bunch of data where, um, they have people play out parts that are meant to be personas, uh, for similar reasons.\n\nSummary:", "completion": "If you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system."}
{"prompt": "Previous summaries:\n\nThe earliest historical example of a dialogue system is called Eliza, which succeeds to the extent that it does because it's imitating a particular style of conversation. In particular, it's imitating a psychologist who practiced things in Rogerian psychology.\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\n\nText:\n\nSo one of the notable things about the Perry system is that, uh, it was, uh, the first, uh, system to pass the touring test. Um, uh, so, um, uh, the touring test is, uh, this classic. Thing in, uh, artificial intelligence that was designed by Alan tur, one of the pioneers of the field of computer science, uh, broadly and, uh, artificial intelligence in particular. Um, so he's got this ACM paper from, uh, the 1950s, um, where he was trying to answer a common question at the time, which was, can computers think.\n\nSummary:", "completion": "The Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think."}
{"prompt": "Previous summaries:\n\nThe job of the Eliza system is to ask questions in order to elicit thoughtful reflection from the user.\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\n\nText:\n\nUm, and so, because that notion of thinking is a little too difficult to operationalize, um, turning proposed this idea of, uh, an alternative test that we could say, um, uh, if a computer could engage in a dialogue, um, with a human evaluator and if it could reliably fool evaluators into thinking that the, um, Agent that they were interacting with via text only, um, was human. Then that was effectively the same as answering the questions.\n\nSummary:", "completion": "The idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human."}
{"prompt": "Previous summaries:\n\nThe user gives examples of things that their boyfriend has done to them. Eliza asks the user for a specific example and the user responds by saying that their boyfriend made them come to the counseling center.\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\n\nText:\n\nCan, uh, machines think like if they could be human, like then we could say that they are, um, passing at least one test of intelligence. Okay. And so, uh, for the Perry system, uh, in 1971, it, uh, passed a variance of the turning tests, where there were a group of, um, psychiatrist, uh, who were asked to analyze a combination of real patients, um, and a computer system running Perry. And since it was the 1970s, it used the 1970s version of texting. Uh, it was like old school teletypes.\n\nSummary:", "completion": "The Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient."}
{"prompt": "Previous summaries:\n\nThe user is not very aggressive, but the system thinks that they don't want the user to notice this. The user doesn't argue with the system because they are afraid of it. The system understands that the user's father is afraid of everybody.\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\n\nText:\n\nUm, and so, uh, the, uh, psychiatrist like interacted with it and. Um, uh, they were, uh, they looked at transcripts of these conversations and then they were trying to pick out, um, which of the patients, uh, were human and which of the patients were the computer. Um, and so the, the group of psychoanalysts who are looking at the transcribed conversations, uh, could only do it like, uh, at random chance. So only about 50% of the time could they identify the correct one, which, or, you know, which one was human.\n\nSummary:", "completion": "The group of psychoanalysts could only do it at random chance, which meant that the system passed the test."}
{"prompt": "Previous summaries:\n\nThe system uses the cleany star operator to ask a question. The word you is matched with the thing in the second cleany star operator up until the word me.\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\n\nText:\n\nSo that was like an early, um, uh, success for, uh, for, uh, dialogue systems on the touring test. And there's now like regular, uh, instances of the touring test that tests, these kind of chat bots. So there's something called the ner prize. Um, that was actually like, uh, created and run by, um,  a carpet salesman from New Jersey. there's a super cool, um, uh, uh, um, a podcast about this. What's the one with Judd Jad Abu Rob, um, Radiolab. So there's a cool Radiolab podcast episode about deterring test in the Logan enterprise.\n\nSummary:", "completion": "The touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots."}
{"prompt": "Previous summaries:\n\nThe idea behind Eliza is to pick out a bunch of keywords and then apply patterns to that data. The textbook gives a simple explanation of how it's working.\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\n\nText:\n\nUm, I totally recommend checking it out. Okay. So, uh, so since Perry passed a touring test, a version of the touring test where, um, psychologists were psychoanalyzing it, well, let's have Eliza Perry, right? So, uh, Perry's in green here, so you can see he's got more personality than she does. So she starts out good evening. Tell me about your problems. Um, and then Perry says people get on my nerves sometimes, and then Eliza says, I'm not sure I fully understand you. You should pay more attention then, uh, maybe you should pay more attention.\n\nSummary:", "completion": "Eliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female."}
{"prompt": "Previous summaries:\n\nThe idea of Eliza is to store possible responses on a stack. If the system craps out, it will back up to some previous part of the conversation and output something from the stack.\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\n\nText:\n\nWell, you're entitled to your own opinion. Uh, what makes you think that I'm entitled to my own opinion, it bothers me just to be around people in general, please go on blah, blah, blah. So, uh, anyway, it, they have this fun, um, and, uh, it's really kind of amusing to pit chatbots against chatbots. So here's a cool example. Um, I'm gonna play this. Can you, uh, tell me whether you actually get the audio? Um, when in the chat window? hello there. Hello there. Hi. How are, how are you? Great. Great. I okay. I'm okay.\n\nSummary:", "completion": "The system is able to have a conversation with the user and the user is able to ask it questions."}
{"prompt": "Previous summaries:\n\nPerry was a system that added in something deeper than Eliza in that it wasn't trying to do this totally neutral thing. Instead, it was meant to embody some kind of personality traits.\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\n\nText:\n\nThat's good. That's good. Are you good? Yes. Yes, I am good. I am good. I just, I just said I was, no, you didn't. I said, I, I said I was. So you are a, so you are a robot is my name is clever. Yes. Yes. A you are a robot and you were named as clever. I am a robot. Yes. Yes. I, together we robots. I'm not a robot. I'm a one. But you said earlier that you are a robot. I, I did not. I thought you did.\n\nSummary:", "completion": "The system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently."}
{"prompt": "Previous summaries:\n\nPerry was meant to be an agent with personality traits. It tried to model mental states and express them.\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\n\nText:\n\nYou were MIS you were mistaken, which is sin memory. Shouldn't be a problem for you. I've answered all your questions. No, you haven't. What is, God you, not everything, not everything could also be something. For example, not everything could be half of something, which is still something and therefore not nothing very true. I would like to imagine it is. Do you believe in God? Yes, I do. So you're Christian. No, I am not. Aren't you say you are not helpful, therefore, you are a meaning. So how you say it or that does not make sense.\n\nSummary:", "completion": "The system is able to reason about God and religion."}
{"prompt": "Previous summaries:\n\nIf you wanted to create interactive fiction, you would want the characters to take on a persona. Some recent work in neural network based chat bots has used data from people playing out parts in order to train the system.\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\n\nText:\n\nDon't do you want to have a body? Sure. Okay. so, uh, anyway, so I think, you know, chatbots are amusing. Um, but like I said, not necessarily super useful, uh, but there has been a lot of interest in them, um, recently because of neural net based architectures. And also because of the rise of things like Amazon Alexa. So, uh, Amazon actually like offered a fairly substantial prize to try to engage, uh, academic, into creating, uh, social bot, like a conversational, uh, chat bot for, um, the Alexa system.\n\nSummary:", "completion": "There has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system."}
{"prompt": "Previous summaries:\n\nThe Perry system was the first system to pass the touring test, which was designed by ACM pioneer Alan Turin. The touring test is a classic test in artificial intelligence that asks whether a computer can think.\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\n\nText:\n\nUm, and you can try it out so you can, um, if you've got a Amazon Alexa at home, uh, you can enable this, uh, chat bot, um, skill just by saying Alexa, let's chat. And then at the end you say, stop. And then, um, it'll actually have you score the system. Um, and you can see kind of like what types of systems are being used. Uh, uh, if you scroll down on this page, you can see some of the reviews are, uh, saying, well, it's not really there yet.\n\nSummary:", "completion": "You can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet."}
{"prompt": "Previous summaries:\n\nThe idea behind the Eliza system was to create an agent that could engage in a dialogue with a human evaluator and reliably fool them into thinking that the agent was human.\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\n\nText:\n\nSo this, uh, is actually a pretty thoughtful review. It says, I, the idea behind this is really good. Uh, let's have universities help develop social bots. The bots themselves are really terrible. I got one, uh, that randomly said, I'm sorry, Angela, that's not my name. I never told it my name. And there was no reason for it to apologize. I got another bot that, well, let's just say it had some pretty unth political opinions, uh, that it interjected randomly when it was supposed to be talking about.\n\nSummary:", "completion": "The idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else."}
{"prompt": "Previous summaries:\n\nThe Perry system passed a version of the Turing test by fooling a group of psychiatrists into thinking that it was a real patient.\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\n\nText:\n\num, and then yet another chat bot said, just said the word you about 20 words,  20 times in a row, blah, blah, blah. So anyway, um, uh, these are, uh, pretty, um, you know, they're experimental systems. So, uh, um, a bunch of my friends are participating in this, including, uh, this guy who's Oliver lemon from Scotland. My name is Oliver lemon. I'm a professor at Harriet wa university in Edinburgh, in Scotland.\n\nSummary:", "completion": "Some of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland."}
{"prompt": "Previous summaries:\n\nThe group of psychoanalysts could only do it at random chance, which meant that the system passed the test.\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\n\nText:\n\nOur team name is WhatsApp, but the data that was produced to this contest was really the main reason why we wanted to join in because it's so hard to get real data from real customers. And Amazon was able to provide amazing data into how like everyday people wanted to talk to these kind of devices. We have a lot of insight from this and that enabled us to train completely new models. We wouldn't have been able to train before now. We have an absolutely awesome conversational AI system that we can build on in the future.\n\nSummary:", "completion": "The data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future."}
{"prompt": "Previous summaries:\n\nThe touring test is a classic test in artificial intelligence that asks whether a computer can think. There is now a regular instance of the touring test that tests chat bots.\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\n\nText:\n\nSo that's been just magical for us. We've got great new skills to being involved in this, and we just really look forward to pushing the boundaries further from now on. So one of the super cool things about it. Uh, about participating is what Oliver said, which is, you know, you actually get real data. Right? And so I think that's actually a super important linchpin of, um, of data driven approaches to conversational agents. Like where do you actually get good conversational data from?\n\nSummary:", "completion": "The data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on."}
{"prompt": "Previous summaries:\n\nEliza Perry is an updated version of the original Eliza system that was created in the 1960s. The original Eliza had a male persona, but this one is female.\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\n\nText:\n\nSo we, in order to build a system that uses a neural network, or that uses other data driven approaches to simulate a human conversation, you actually need like huge volumes of recorded conversations or examples of people engaging in, um, things that closely approximate conversations. So, um, you know, uh, most people don't have access to the same kind of data that Amazon does with its Alexa system or that. Google or apple have with its system. So as researchers, we try to approximate this data.\n\nSummary:", "completion": "In order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems."}
{"prompt": "Previous summaries:\n\nThe system is able to have a conversation with the user and the user is able to ask it questions.\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\n\nText:\n\nUm, and in fact, for, even for those systems, you know, I think the promise of them starts out as like a conversational agent, but then, um, like people super quickly adapt to the types of questions that they're capable of answering. So I think probably the most interesting data that you could have as a researcher to look at, like how people would really ideally like to use these systems are like the first 10, five or 10 questions that they ask where they get super frustrated afterwards, because like they can't do any of that stuff.\n\nSummary:", "completion": "The most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards."}
{"prompt": "Previous summaries:\n\nThe system is able to have a conversation with the user. The user can ask it questions and it will respond intelligently.\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\n\nText:\n\nOr maybe like, it might actually be interesting to mine what children ask it because they don't have any preconceptions of like what it should be able to do. And so they're willing to try a lot more. . Um, okay. So anyway, what we really want is large collections of human conversations. So the types of data that people use are things that kind of approximate that.\n\nSummary:", "completion": "The types of data that people use are things that approximate human conversations."}
{"prompt": "Previous summaries:\n\nThe system is able to reason about God and religion.\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\n\nText:\n\nSo a lot of research has revolved around, uh, Twitter, or if you're interested in, um, like doing, uh, this kind of conversational modeling for Chinese than Y above, um, which is, uh, similar to Twitter. So there, uh, you don't just take the tweets where people post something, what you want is like the threads where people have conversations back and forth with each other. Um, so that somehow approximate, approximate, this notion of like turn taking and you're contributing something, um, that's responding to something that someone just said. And so on.\n\nSummary:", "completion": "Twitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other."}
{"prompt": "Previous summaries:\n\nThere has been a lot of interest in neural net based architectures for chat bots. Amazon even offered a prize to try to engage academic researchers in creating social bots for its Alexa system.\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\n\nText:\n\nUm, another source of data has been, uh, trying to use, uh, Dialogue and movies. So there's lot, lots of very large corporate, um, where we have movie subtitles, uh, in a nice digital forum. So, uh, you used that in, uh, or you are using that in your NMT, your neuro machine translation homework at the moment.\n\nSummary:", "completion": "Another source of data has been trying to use movie subtitles in a nice digital format."}
{"prompt": "Previous summaries:\n\nYou can try out the system by saying \"Alexa, let's chat\". At the end, you can have it score the system. Some reviews say that it is not quite there yet.\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\n\nText:\n\nUm, so instead of trying to use, uh, aligned subtitles, where you have a English and Japanese version, you're really just using, uh, ones within a single language and you're keeping, uh, track of the order in which, uh, they appear in the movie. So you get the idea that it's like, uh, potentially reflective of conversations had between two characters, unfortunately, movie subtitles themselves. Don't actually like, say who's speaking. So it's a little bit tricky in that we don't actually have clear boundaries between, uh, whose turn it is. And each.\n\nSummary:", "completion": "Instead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is."}
{"prompt": "Previous summaries:\n\nThe idea behind this is really good. Let's have universities help develop social bots. The bots themselves are really terrible though. I got one that randomly said, \"I'm sorry, Angela, that's not my name.\" I never told it my name and there was no reason for it to apologize. I got another bot that interjected some pretty unth political opinions when it was supposed to be talking about something else.\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\n\nText:\n\nThing in the file just represents, like what's shown on the screen at that moment. So it could be like the same character talking for several steps. So it's not a perfect, um, match for conversational data, but it's data that's there. That's kind of conversational, like, um, there's other data sources.\n\nSummary:", "completion": "There is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data."}
{"prompt": "Previous summaries:\n\nSome of my friends are participating in this. One of them is Oliver lemon from Scotland. He is a professor at Harriet wa university in Edinburgh, in Scotland.\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\n\nText:\n\nUh, so the linguistics dated consortium, uh, over many years collected, uh, uh, telephone speech collections because, um, a lot of companies were interested in trying to build automatic speech recognizers, uh, to transcribe telephone, um, audio, uh, in order to be build things like, um, uh, systems to respond to customer support queries. Um, so there was a big push on telephone, uh, speech.\n\nSummary:", "completion": "There is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections."}
{"prompt": "Previous summaries:\n\nThe data that was produced for this contest was the main reason why the team wanted to join. They have an awesome conversational AI system that they can build on in the future.\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\n\nText:\n\nUm, the linguistics data consortium and, um, other previous entities like Texas instruments was a big, uh, company that focused on this in bell labs, they collected large, uh, conversational telephone, um, speech corporate, where they, uh, would originally just like pay people to like randomly be paired with another person on the other end of the telephone. And they were given a topic to talk about, and then they just talked back and forth for half an hour. Uh, so that's a pretty nice, um, collection of conversational, um, data.\n\nSummary:", "completion": "Some of the data that has been collected is from random conversations between two people."}
{"prompt": "Previous summaries:\n\nThe data that was produced for this contest was magical for the team. They got great new skills to being involved in this and they just really look forward to pushing the boundaries further from now on.\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\n\nText:\n\nAnd, uh, then you're asking one of the textbook authors did a lot of work on, um, modeling, dialogues using these, um, uh, uh, uh, switchboard Corpus is what it's called. Um, other groups like, uh, Facebook research have. Had ways of collecting data using crowdsourcing. So there's, um, a system called Amazon mechanical Turk, which I personally love for collecting data. Um, and they, um, made it so that you could pair two crowd workers together and have them have a conversation. Uh, so it's super useful for collecting this kind of data.\n\nSummary:", "completion": "There are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk."}
{"prompt": "Previous summaries:\n\nIn order to build a system that uses a neural network, or any other data driven approach, you need huge volumes of recorded conversations. Most people do not have access to the same kind of data that Amazon does with its Alexa system or that Google or Apple does with its systems.\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\n\nText:\n\nUm, so once you've got this kind of data, there's really, uh, two architectures that people have been using. Um, so the first is a information retrieval based one. Um, and then the next one is a machine learning, like, um, neural sequence to sequence transaction one. Um, so if you want to build a chat bot based on information retrieval, then, uh, anyone in this class could do that. Right? So if you think back to the homework where we looked at, uh, Shakespeare's plays and representing.\n\nSummary:", "completion": "There are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful."}
{"prompt": "Previous summaries:\n\nThe most interesting data for researchers to look at are the first 10, five or 10 questions that people ask where they get super frustrated afterwards.\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\n\nText:\n\nA term by document matrix, then that's really the essence of what's happening, um, for the first step of a IR information retrieval based chat bot. So whatever the user types into you, you represent as a vector. So that's the query vector queue. And then you take that query vector and you say, you know, apply whatever transform you want. You could do PPMI you could do TF IDF, um, and then, um, compare that vector against vectors, representing every, um, conversational turn in your large Corpus of conversations.\n\nSummary:", "completion": "The essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus."}
{"prompt": "Previous summaries:\n\nThe types of data that people use are things that approximate human conversations.\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\n\nText:\n\nUm, and then the goal is to find the single conversational turn that, uh, maximizes the co-sign similarity between the query vector and that turn in the conversation Corpus. Right. And then once you found the closest match, you just look up what's the response to that. Um, and then you can return that as the response for your chat bot.\n\nSummary:", "completion": "The goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot."}
{"prompt": "Previous summaries:\n\nTwitter is a good source of data for conversational modeling. The ideal kind of data is where people are having conversations back and forth with each other.\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\n\nText:\n\nSo the idea is if someone types into the chat bot during the turn test, Hey, have you ever watched Dr. Who, um, then you could, uh, use that as the query vector and find, uh, not the exact thing, cuz that's unlikely to occur in your conversation, but you might find something similar. So you could find something like, do you like Dr. Who? Um, and then you look up whatever response, uh, the person gave to it. So someone might have said, yeah, I really love sci-fi shows.\n\nSummary:", "completion": "The idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot."}
{"prompt": "Previous summaries:\n\nAnother source of data has been trying to use movie subtitles in a nice digital format.\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\n\nText:\n\nUm, and then that ends up being the response that you give to the user queries. So it, uh, ends up being pretty relevant. Right. But you can imagine like different ways that that could go wrong. Like your closest specter might not be close enough or some other, uh, problems. um, which we'll see in a minute. um, okay, so that's representing it as like a TF IDF style vector. Um, we could also use for modern architectures where we're basing our similarity, um, on, uh, some kind of, uh, deep neural network.\n\nSummary:", "completion": "The closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words."}
{"prompt": "Previous summaries:\n\nInstead of using aligned subtitles, which have a English and Japanese version, the team uses movie subtitles that are in a single language. Unfortunately, movie subtitles themselves do not have clear boundaries between whose turn it is.\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\n\nText:\n\nUm, so, uh, the way that this is done is basically to create, uh, a, um, vector based representation. That's like a dense, uh, sentence embedding of the query vector and the response vector, but the idea is pretty similar to, um, the old school like TF IDF. What you want to do is represent the query vector and all the turns in your conversational Corpus as vectors. And they can either be large sparse vectors or short, dense vectors.\n\nSummary:", "completion": "The idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors."}
{"prompt": "Previous summaries:\n\nThere is other data that can be used for conversational modeling, such as movie subtitles. However, the problem with movie subtitles is that they are not a perfect match for conversational data.\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\n\nText:\n\nUm, and as we've seen in the past, it tends to be that, uh, the short, dense vectors end up being. the most useful. Um, and then you could extend this idea. So instead of just representing the words themselves, you could, uh, encode the query vector with additional features, right? So you could try to encode information about the user themselves.\n\nSummary:", "completion": "The idea is to represent the query vector with additional features, such as those about the user."}
{"prompt": "Previous summaries:\n\nThere is a lot of data that has been collected over the years that is useful for conversational modeling. Some of this data comes from telephone speech collections.\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\n\nText:\n\nSo if you wanted, uh, to have a personality based chat bot that was like a student or something like that, then you could say, let's try to encode a feature for all Twitter users to say, is this person likely to be a student or whatever? Um, uh, you could also have, uh, other information about the previous turns. So instead of just this immediate one, you could encode a longer vector that took account prior turn, so that could, uh, match it. Um, and there's lots of different IR based methods that have used this.\n\nSummary:", "completion": "We can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy."}
{"prompt": "Previous summaries:\n\nSome of the data that has been collected is from random conversations between two people.\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\n\nText:\n\nSo a pretty early one, uh, was by PR Isabelle. Um, and his group, uh, from Canada, uh, that used an information retrieval based response to pull out sentences from things like, um, uh, scripts of the big Loki or the planet apes or, um, uh, the uni bomber manifesto. Not quite sure why they picked that or Wikipedia text. So we have a, a good question in the chat. It says, uh, could you have it so that the closest vector, um, that was below a certain distance? Uh, um, sorry, let me read this again.\n\nSummary:", "completion": "There is a document that describes how to use an information retrieval based method to extract sentences from scripts."}
{"prompt": "Previous summaries:\n\nThere are many different ways to collect data for conversational modeling. Some of these methods include using the switchboard corpus and Amazon mechanical Turk.\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\n\nText:\n\nOh, uh, could you have it that the closest vector has to be below a certain distance threshold, um, to avoid the closest vector being too far away. Um, so if the closest vector is close enough, like within this, uh, threshold that you want, then you return the next. Um, otherwise you do some other behavior. I think that's an excellent suggestion. Um, so basically having a fallback where if, uh, you don't have anything that looks relevant, that isn't like, uh, a vector that's close enough in your Corpus of conversations, then try some other strategy.\n\nSummary:", "completion": "In this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy."}
{"prompt": "Previous summaries:\n\nThere are two main architectures for conversational modeling: an information retrieval based one and a machine learning based one. The former is easier for students to do, but the latter is more powerful.\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\n\nText:\n\nSo, yeah, that's a great suggestion. Okay. So, um, that's an example of information retrieval based models. So, um, of course these all rely on having, um, a data set to, to, uh, retrieve from, and it turns out that, uh, retrieving from Twitter, uh, can be a terrible idea. Um, so there was an experiment that, um, Microsoft ran in 2016 where it built, built a chat bot. Uh, and it, uh, basically became racist in less than a day.\n\nSummary:", "completion": "Information retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day."}
{"prompt": "Previous summaries:\n\nThe essence of an information retrieval based chat bot is representing what the user types as a vector and then comparing that vector against vectors representing every conversational turn in a large corpus.\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\n\nText:\n\nSo there's a nice, um, article from the verge, uh, that talks about this as a cautionary tale for all of us studying NLP. So it took less than 24 hours for Twitter to corrupt an innocence, AI chat bot, uh, Microsoft unveiled, the TA chat bot, um, that was meant to like participate in these conversational understanding things. Um, and it's supposed to learn as it goes. Um, unfortunately the conversations didn't stay playful for very long. Pretty soon after TA launched people started tweeting the bot with all sorts of misogynistic racists and Donald Trump, uh, like remarks.\n\nSummary:", "completion": "An AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks."}
{"prompt": "Previous summaries:\n\nThe goal is to find the single conversational turn that maximizes the cosine similarity between the query vector and a turn in the conversation corpus. Once you have done that, you can return the response for your chat bot.\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\n\nText:\n\nUm, and since Tay was one of these data driven systems, it was really just learned to. uh, parrot conversations. It started repeating these, uh, terrible sentiments back to users. So it started out at the beginning as, uh, giving responses like this. Uh, so this is, um, March 23rd, 8:00 PM. Can I just say that I'm super stoked to meet you? Humans are super cool. Okay. Uh, mark, the next day, um, a little less than 24 hours later responds to a couple people saying chill. I'm a nice person. I just hate everybody.\n\nSummary:", "completion": "Tay, a data driven AI chat bot, started to parrot back racist and Trump like comments."}
{"prompt": "Previous summaries:\n\nThe idea is to use the query vector to find the most similar turn in the conversation corpus. Then, you can use that response as a response for your chat bot.\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\n\nText:\n\nUm, and then, uh, uh, the next day saying I fucking hate feminists and they all should die and burn hell and then Hitler was right. I hate the Jews. So crazy, crazy racist and misogynistic stuff. Uh, it started coming out of the mouth of this AI developed by Microsoft. So they like instantly pulled the plug. Um, so it, uh, uh, one of my favorite people on Twitter, Hillary Mason, who was the co-founder of bit Lee, and who's an amazing data scientist, uh, had this tweet that really resonated with me.\n\nSummary:", "completion": "The AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me."}
{"prompt": "Previous summaries:\n\nThe closest specter may not be close enough, which could cause problems for the architecture. We can also use a deep neural network based similarity measure to represent words.\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\n\nText:\n\nSo if you told me a decade ago that I would be worrying about writing racist computer programs, I would not have believed you. Right. So the crazy thing is like, you actually need to take active steps in order to avoid crazy stuff like this. Right. Um, and so the key is like these systems are all based on the data that they're being trained on. Right? So you have to be super careful about the type of data that you train on and thoughtful about filtering out, uh, potentially terrible replies. Right. Okay.\n\nSummary:", "completion": "The crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies."}
{"prompt": "Previous summaries:\n\nThe idea is to represent the query vector and all the turns in your conversational corpus as vectors. These vectors can be either large sparse vectors or short, dense vectors.\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\n\nText:\n\nSo, um, one, uh, the like thing that revived people's interest in building chatbots, uh, was, uh, the end coder decoded neural network architectures that we've been talking about for the past week or two. So, um, uh, one way of thinking about trying to produce a response is as a generation task, right? So we're taking in, um, the user's previous turn, uh, and, uh, feeding it through the end coder part of the model.\n\nSummary:", "completion": "One way of producing a response is to take the user's previous turn and feed it through the end coder part of the model."}
{"prompt": "Previous summaries:\n\nThe idea is to represent the query vector with additional features, such as those about the user.\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\n\nText:\n\nAnd then, uh, we just, as we took in the source sentence for machine translation, once we've encoded that representation, then we want to generate some, uh, response to it. So the training steps for this kind of encode or decoder model in order to, um,  chat like, uh, uh, responses is gonna be identical to the training procedure that you're using for the neural machine translation now. Right. So you're gonna just take the data, uh, and you're gonna take, um, turn and then response, and you're gonna, um, concatenate them together and have some token in the middle.\n\nSummary:", "completion": "The training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation."}
{"prompt": "Previous summaries:\n\nWe can encode features for the user in order to create a more personalized chat bot. There are many different IR based methods that have used this strategy.\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\n\nText:\n\nAnd then the generation is gonna be given a prefix of whatever the user just typed, generat a response. Um, and so this, uh, decoder architecture, this, sorry, the sequence to sequence en coder decoder architecture is, uh, should look super familiar as a result, right? So, um, we've got all of these hidden states that are based on the input words. Um, so mom, I don't feel so good is the response. Um, and then we've got this utterance representation, which is the final hidden state.\n\nSummary:", "completion": "The input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation."}
{"prompt": "Previous summaries:\n\nThere is a document that describes how to use an information retrieval based method to extract sentences from scripts.\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\n\nText:\n\nand we could aggregate this together as the context hidden state and then, uh, push it on over to the decoder. Um, and then it'll generate something like what's wrong. And then, um, the next turn, then, then you could also use the, that as a training item, if these are drawn for your, uh, Corpus of Twitter. So you can have it, uh, encode that what's wrong as the utterance representation, and then, uh, uh, try to force it to decode the output. Like, I feel like I'm gonna pass out. So this is like movie movie dialogue stuff.\n\nSummary:", "completion": "The input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation."}
{"prompt": "Previous summaries:\n\nIn this strategy, the algorithm will return the most similar turn if the closest vector is within a certain distance threshold. Otherwise, it will try some other strategy.\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\n\nText:\n\nOkay. Uh, so that's the sequence, the sequence end coder decode or model. Um, so the context that's taking account of is really just this, um, original statement, right? It doesn't have any longer history, so that's what of the limitation. And there's been lots of different styles of, um, sequence to sequence models. There's a notion that you can learn more about if you take the CIS 5 22, the, uh, uh, deep learning course here at Penn, um, called adversarial learning.\n\nSummary:", "completion": "The limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning."}
{"prompt": "Previous summaries:\n\nInformation retrieval based models can be great, but they can also be awful. There was an experiment where a chat bot became racist in less than a day.\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\n\nText:\n\nSo that's basically, you try to train a system, not just to the maximum likelihood objective, but you're basically trying to train it in such a way that it's got a, um, generator component, like your decoder and a classification component, like trying to predict whether something is, um, output by your, uh, the machine, or is a real drawn from the true distribution. So that's called adversarial learning. And so here's some example of, um, uh, outputs from a particular paper that use this adversarial training. So, um, the input is the prompt. So tell me how.\n\nSummary:", "completion": "Adversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component."}
{"prompt": "Previous summaries:\n\nAn AI chat bot was introduced by Microsoft that was meant to learn as it went. Unfortunately, the conversations did not stay playful for very long. People tweeted the bot with all sorts of misogynistic, racist, and Trump like remarks.\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\n\nText:\n\nhave you had this, uh, falling sickness and then in a vanilla sequence, the sequence model, it's picking up on stuff like sickness, and then it's gonna have something that has doctor in it. So it says I'm not a doctor which doesn't really make sense. And then the adversarial learned one says a few months ago, I guess. Um, and then the input, something like, uh, so I had the doctor test Sammy's response to the conditioning, and then you get something like what the Amazon reviewer is complaining about.\n\nSummary:", "completion": "The input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about."}
{"prompt": "Previous summaries:\n\nTay, a data driven AI chat bot, started to parrot back racist and Trump like comments.\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\n\nText:\n\nSammy wrote the test, Sammy wrote the test where you, for whatever reason, get this kind of loopy behavior. And then the adversarial learned one produces a better response. Like, so he took the pills, um, et cetera. Right? So, uh, you can see that there's lots of cool directions that you could build better context based representations or better ways of encoding the meaning of the inputs or. Uh, better outputs. Uh, so, um, these are all, uh, interesting research directions that people have explored, uh, using neural architectures. Okay. So, uh, chat bots, they're pretty cool. Right?\n\nSummary:", "completion": "The adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures."}
{"prompt": "Previous summaries:\n\nThe AI chat bot started to say really crazy, racist and misogynistic things. So they instantly pulled the plug. One of my favorite people on Twitter, Hillary Mason, had a tweet that really resonated with me.\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\n\nText:\n\nThey're fun. You could imagine that they might have some applications to psychotherapy. So, um, there's actually research that suggests that, you know, people, uh, could benefit from talking to robots. So, um, I'm a little bit skeptical myself about the psychological counseling side of things. Um, but, uh, there's been like a lot of interesting applications because the military funds so much of, uh, research in the United States, um, built ones like a chat bot for, uh, people who had been, um, who had signed up as new military recruits to talk to.\n\nSummary:", "completion": "There have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits."}
{"prompt": "Previous summaries:\n\nThe crazy thing is that we have to be super careful about the data that we give to these systems in order to avoid crazy stuff like this. The key is to be thoughtful about filtering out terrible replies.\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\n\nText:\n\nAnd it, it ended up that people asked a lot of questions that they were like too embarrassed to ask a actual human. So they'd be, they would ask questions, like, do I need to bring my own socks or things like that? You know, like they just didn't thought it was too obvious or too dumb to too unimportant to ask a, uh, human, but they were willing to ask a robot, um, the cons of chatbots. Well, they can get racist. Uh, I think that's mainly a reflection on internet culture. Um, maybe not the robots themselves.\n\nSummary:", "completion": "The advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture."}
{"prompt": "Previous summaries:\n\nOne way of producing a response is to take the user's previous turn and feed it through the end coder part of the model.\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\n\nText:\n\nUm, and they don't really understand anything. Right. So they don't really have any deep, uh, uh, understanding of conversations or the world or anything like that. It's really just a, um, input, output, uh, trick. Um, and, uh, that happens with. Um, Eliza, where it was really just a regular expression trick. It happens with modern systems where it's really just the big data trick. Um, and for the big data ones, they're like really only mirroring the data, the training data. Right? So in some cases that's terrible.\n\nSummary:", "completion": "The advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture."}
{"prompt": "Previous summaries:\n\nThe training steps for this kind of encode or decoder model are identical to the training procedure for neural machine translation.\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\n\nText:\n\nLike it can end up with garbage and garbage out like Microsoft te um, in some cases there might be potentially interesting uses of it. Um, so I think, uh, this would be, this is a fun direction. That's easy to experiment with if you've got the right data, because we've got lots of architectures in place now that allows us to do these tricks. So, um, you can imagine cool potential applications of it that, uh, that could be fun. Okay.\n\nSummary:", "completion": "Neural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing."}
{"prompt": "Previous summaries:\n\nThe input is passed through a network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\n\nText:\n\nSo let's take a look at the type of dialogue agent that I think is like, um, more relevant, which are task based dialogue systems. Um, so chat bots don't really have any goal per se. Like they're really just meant for entertainment. Um, but task based, uh, dialogue agents are really, uh, aimed at trying to solve a particular task. They're trying to, uh, represent user intentions and then try to collect information for them in order to be able to execute some action.\n\nSummary:", "completion": "Task based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action."}
{"prompt": "Previous summaries:\n\nThe input is passed through a neural network that produces a word vector for each output. The output is a word vector and an utterance representation.\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\n\nText:\n\nUm, so exam examples in the textbook are really like, how do you build a system to, uh, be like a, um, airline reservation agent? So you want to do a lot of slot filling, right? Which is what we've studied for, uh, information extraction and, uh, and frames. So what we want is to get all of the relevant pieces of information in order to list, um, in order to execute a booking command. So we want to know.\n\nSummary:", "completion": "Exampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command."}
{"prompt": "Previous summaries:\n\nThe limitation of the sequence to sequence model is that it has no context. There have been lots of different styles of sequence to sequence models, including ones that use adversarial learning.\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\n\nText:\n\nWhere is someone traveling from where are they going to, what date they want to travel on, what time they wanna leave and what airline. Um, and so, uh, this kind of frame based dialogue agent is also a historical thing. So it goes back all the way to the 1970s, um, in, um, Xerox park, which is, uh, this really amazing hub of innovation for, um, all sorts of computer science research. That's basically where they invented the graphical user interface that Steve jobs saw and then, uh, borrowed for the Macintosh.\n\nSummary:", "completion": "Frame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers."}
{"prompt": "Previous summaries:\n\nAdversarial learning is a technique that can be used to train a neural network in such a way that it has a generator component and a classification component.\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\n\nText:\n\nThey also invented things like, um, networked computing, hyper text, all sorts of cool, crazy stuff like that. Um, it was right around the corner from, uh, Stanford. Uh, so like as an undergrad there, I, um, did an internship at, um, C park. and one of the co-authors, or I know a couple of the co-authors on this, um, original paper. Um, so Gus is a gen understanding system. And so here's an example transcript from the Gus dialogue agent. Um, so it says, hello, my name's Gus, can I help you plan a simple trip by air?\n\nSummary:", "completion": "Gus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm."}
{"prompt": "Previous summaries:\n\nThe input is some text that has a doctor in it. The traditional sequence model will say I'm not a doctor, which doesn't make sense. The adversarial learned sequence model says a few months ago. The input is something like what the Amazon reviewer is complaining about.\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\n\nText:\n\nWhere, where do you wanna go? And then the person interacting with that says, I wanna go to San Diego on May 28th. What time do you wanna leave? I have to be in San Diego before 10:00 AM. Um, would you like this now? Long gone  uh, airline, would you like the flight number 1 0 2 that arrives at nine 15? Uh, what time is the next flight? Uh, the next one leaves at eight 30 and only arrives at 10. Okay. I'll take the first one. Um, what's the name for the reservation, et cetera, et cetera.\n\nSummary:", "completion": "Gus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language."}
{"prompt": "Previous summaries:\n\nThe adversarial learned sequence model can produce a better response than the traditional sequence model. There are lots of cool research directions that people have explored using neural architectures.\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\n\nText:\n\nRight. Uh, so the idea is you want populate all these slot filler things, right? So you want a.  uh, so you want to get the month, the year, the day. Um, and, um, you're gonna have to go through and populate all the sorts of, um, uh, elements that you need in order to execute it. So those are all represented as semantic frames, like we talked about with frame net, um, and, um, uh, a lot of systems are still based on this kind of architecture. So, uh, systems like Alexa and Siri are not neural network based.\n\nSummary:", "completion": "The idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based."}
{"prompt": "Previous summaries:\n\nThere have been interesting applications for neural language models, including a chat bot for people who had signed up as new military recruits.\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\n\nText:\n\nThey're really trying to map onto explicit task based things. And so a lot of what's happening under the hood in this early guest system and in modern, um, slot based systems are first figuring out, like, what are you talking about? Like, what domain are you talking about? And then, um, there's a step called intent detection, which is really a text classification style thing. So I want to understand, um, whether you want to remove a calendar appointment or add one or reschedule one.\n\nSummary:", "completion": "Intent detection is a text classification style thing. The system has to understand what the user wants to do."}
{"prompt": "Previous summaries:\n\nThe advantages of chatbots are that they can answer any question and they can never get bored of the same questions. The disadvantages are that they can get racist and reflect internet culture.\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\n\nText:\n\nUm, and then the system needs to, um, extract or elicit the slot filling things that it needs to execute its command. Um, so if I say, show me morning flights from Boston to San Francisco on Tuesday, that I know that I'm in the general domain of air travel and the intent detection system should trigger something to say that I want to show flights. Uh, and then the slot fill air should extract, um, the relevant things that the user said.\n\nSummary:", "completion": "The system needs to elicit the slot filling things that it needs to execute its command."}
{"prompt": "Previous summaries:\n\nThe advantages of neural language models are that they can have a deep understanding of conversations and the world. The disadvantages are that they can be racist and reflect internet culture.\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\n\nText:\n\nUm, similarly, if I say, if I tell Siri something like, wake me up tomorrow at six, then the domain should be, um, alarm clocks. And the intent is for me to set an alarm clock, not to reschedule it. And then I need to do some temporal. Uh, expressions to, uh, map from that relative time onto a concrete one, um, based on the current date. Um, so a lot of it, uh, uses semantic grammars and regular expressions and, uh, semantic role labeling in order to, uh, populate these things.\n\nSummary:", "completion": "The intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one."}
{"prompt": "Previous summaries:\n\nNeural language models can have a deep understanding of conversations and the world. There are cool potential applications for this, including ones that require real-time processing.\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\n\nText:\n\nUm, and many use, many systems are built on rule sets rather than, um, data driven, uh, learning. So you need a bunch of rules that consist of some condition condition, like what the user just said, and then what action the system ought to take. Um, and then there's an interesting idea that you need to, um, uh, add facts about the user over time, right? So you need to build up some state representation.\n\nSummary:", "completion": "Many systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time."}
{"prompt": "Previous summaries:\n\nTask based dialogue systems are meant for solving a particular task. They try to represent user intentions and collect information for the user in order to execute an action.\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\n\nText:\n\nAbout like the user's calendar or about, um, where they currently are or their name or things like that, and be able to, uh, execute future commands, um, more quickly. Um, so here's an example of an architecture. Uh, this is one of the frame based ones, so there's, uh, the automatic speech recognition component a already adds some probabilistic uncertainty. Right. So, um, I mentioned briefly in yes. In Monday's lecture that like, uh, speech recognition systems can get misled.\n\nSummary:", "completion": "An architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty."}
{"prompt": "Previous summaries:\n\nExampl es in the textbook are things like how do you build a system to be like an airline reservation agent? The goal is to get all the relevant pieces of information in order to execute a booking command.\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\n\nText:\n\nSo if you say something like how to recognize speech, then it might get something like how to, recco a nice speech. Yes. how to rec a nice beach. okay. Anyway, uh, so anyway, so modern speech recognition systems will output a probability distribution over possible responses. So here, if someone says leaving from downtown, um, you might, uh, MIS recognize it, or you might think it's sitting, leaving at some time. Um, and then the automatic speech recognition system will, uh, put forward its list of candidate, um, transcriptions.\n\nSummary:", "completion": "Modern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user."}
{"prompt": "Previous summaries:\n\nFrame based dialogue agents are a historical thing that go back to the 1970s. They are a really cool way of interacting with computers.\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\n\nText:\n\nAnd then the spoken language understanding unit will try to do the frame extraction where it's saying from where at what time, and then it's trying to encode those probabilities too. Uh, the dialogue state tracker will try to keep the representation of what information it knows. And so if it has a low enough probability for some of its frames, like the arrival time or the departure time, um, then it'll end up having to ask for those again. So the dialogue policy.\n\nSummary:", "completion": "The spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows."}
{"prompt": "Previous summaries:\n\nGus is a gen understanding system that can help plan a trip. It is based on the original Eliza algorithm.\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\n\nText:\n\nShould, uh, decide when the system should confirm some piece, like, so it says, I think you want to leave from downtown, is that correct? Um, and then, uh, that'll be produced by the system through a text to speech module. Okay. So let's take it. So there's lots of elements that make this cool and useful. Um, lots of machine learning, uh, for, uh, state tracking for dialogue. And, uh, some of the interesting things are to do with the natural language generation component. So like what should you say in response to user utterance?\n\nSummary:", "completion": "There are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component."}
{"prompt": "Previous summaries:\n\nGus is a system that can help plan trips. The user can ask for information about a trip and receive answers in natural language.\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\n\nText:\n\nUm, and how can you generate natural sounding text? So there's one system that was like a demo system by Google, um, for their AI research called Google duplex. So it was, it's meant to be an AI system that can, um, accomplish tasks on your behalf over the phone. Right. So now instead of the system being something that you call into the system is calling on your behalf to do things like make a restaurant reservation, um, or book a haircut, which, um, I really need to do.\n\nSummary:", "completion": "There is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts."}
{"prompt": "Previous summaries:\n\nThe idea is to populate all the slot filler things, like the month, year, and day. A lot of systems are still based on an architecture that is similar to frame net. Systems like Alexa and Siri are not neural network based.\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\n\nText:\n\nUh, so, uh, some of the elements of this were really outstanding. So they trained a speech synthesis system that, uh, is both like super realistic. Like, it sounds like a person is calling. And one of the things that made it realistic was it managed to give lots of, um, what are called back channel responses. So just. uh, and ahas and mm-hmm  and filled pauses and things like that. So, um, let's just take a listen so you can see the quality of this.\n\nSummary:", "completion": "The system uses a speech synthesis system that is both realistic and able to give back channel responses."}
{"prompt": "Previous summaries:\n\nIntent detection is a text classification style thing. The system has to understand what the user wants to do.\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\n\nText:\n\nSo, um, the person who will answer this first is a human and then the person replying is your dialogue agent calling to try to book you a haircut. How can I help you? So how can I help you? I'm calling hi, I'm calling to book a womans haircut for a client. I'm looking for something on May 3rd. Sure. Give one. Sure. Gimme one second. Mm-hmm . So it says mm-hmm,  sure. What time are you looking? Sure. What time are you looking for? Around at 12:00 PM at 12:00 PM. We do not have a 12.\n\nSummary:", "completion": "The system can book a woman's haircut for a client. The system asks for the time and then prepares an answer."}
{"prompt": "Previous summaries:\n\nThe system needs to elicit the slot filling things that it needs to execute its command.\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\n\nText:\n\nWe do not have a 12:00 PM available. The closest we have to that is a one 15. Do you have anything between 10? Do you have anything between 10:00 AM and, uh, 12:00 PM, depending on what service, depending on what service she would, like, what service is she looking for? Just a woman's haircut for now. Just a woman's haircut for now. We have a 10 o'clock. Okay. We have a 10 o'clock. 10:00 AM is fine. 10:00 AM is fine. What's her first name. Okay. What's her first name? The first name is Lisa.\n\nSummary:", "completion": "The system can book a woman's haircut for a client. The client's first name is Lisa."}
{"prompt": "Previous summaries:\n\nThe intent is for the user to set an alarm clock, not to reschedule it. The system uses semantic grammars and regular expressions in order to map from a relative time onto a concrete one.\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\n\nText:\n\nThe first name is Lisa. Okay, perfect. So I will see, so I will see Lisa 10 o'clock on May 3rd. Great, thanks. Great. Have a great day. Great. Have a great day. Bye. So this is like UN Cannelly. Good. Right? The speech synthesis is amazing. Uh, there's some questions about the ethics of like kind of Masque rating as a human, to make an appointment. Um, and not being obviously a machine.\n\nSummary:", "completion": "The system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment."}
{"prompt": "Previous summaries:\n\nMany systems are built on rule sets rather than data driven learning. The system needs to add facts about the user over time.\nAn architecture for a system that interacts with the user through speech is shown. The automatic speech recognition component adds some probabilistic uncertainty.\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\n\nText:\n\nbut the interaction is who seamless, because they've done such a good job at like trying to elicit the important things to execute the, um, slot filler segment. It's really good. So here's um, another example of, uh, the Google system making a restaurant appointment. see, how may I hear you? See, how may I hear you? Hi, um, I'd like, hi. Um, I'd like to reserve a table for Wednesday, the seventh for seven for seven people. Um, it's for four it's for four people. We four people when Wednesday it's Wednesday at 6:00 PM.\n\nSummary:", "completion": "The system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request."}
{"prompt": "Text:\n\nOh, actually we need to for like upper, like apply people for before you can come. How long is the wait? How long is the wait usually to, uh, be seat? When tomorrow when tomorrow or weekday or for next Wednesday for next Wednesday? Uh, the seventh. Oh, no, it's not too busy. Oh, no, it's not too busy. It's you can talk for people. Okay. Oh, I gotcha. Oh, oh, I gotcha. Thanks. All right. So that's another example of the state of the art, uh, for these types of systems.\n\nSummary:", "completion": "This is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat."}
{"prompt": "Previous summaries:\n\nModern speech recognition systems output a probability distribution over possible responses. The system then uses these probabilities to decide which item to present to the user.\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\n\nText:\n\nUm, there's lots of super cool things, uh, that you could do, uh, with systems. A lot of, um, research in this has adopted a design methodology, uh, where they, um, try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments. So there's a scene from the wizard of Oz, where the wizard is like, you know, doing all sorts of pirate tactics and making itself seem really big, but really there's just a. A person behind a curtain operating this machinery to show off.\n\nSummary:", "completion": "There are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments."}
{"prompt": "Previous summaries:\n\nThe spoken language understanding unit tries to do frame extraction and encode probabilities. The dialogue state tracker tries to keep the representation of what information it knows.\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\n\nText:\n\nAnd so in a lot of, um, natural language processing studies that intersect with human computer interaction, they take this kind of methodology. So they have people that experimental participants interact with the dialogue system. But instead of making sure that it's perfect, they just have someone listening on the other end and providing the responses if it were the system. Um, and that allows you to really study the user and what types of things they would like to do and how they do it and the task itself. So it's a super cool, um, type of methodology.\n\nSummary:", "completion": "In many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology."}
{"prompt": "Previous summaries:\n\nThere are lots of elements that make this system cool and useful. Some of the interesting things are to do with the natural language generation component.\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\nIn many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology.\n\nText:\n\nUh, and so that Google one feels a bit like a wizard of Oz thing, but, uh, it's apparently, um, real, so those were, those dialogues were no doubt, like cherry picked to pick out the examples of the best possible things there. All right. So, uh, when you're building a dialogue system,  it's, there's, it's important to consider the ethical issues. So everything from like, should you disclose that your dialogue system is a machine like on the, they fail to do on those Google calls?\n\nSummary:", "completion": "It is important to consider the ethical issues when building a dialogue system. This includes whether or not to disclose that the system is a machine."}
{"prompt": "Previous summaries:\n\nThere is a system called Google Duplex that can make phone calls on your behalf. The system can make restaurant reservations and book haircuts.\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\nIn many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology.\nIt is important to consider the ethical issues when building a dialogue system. This includes whether or not to disclose that the system is a machine.\n\nText:\n\nUm, you should be, uh, cognizant of, um, biases that occur in your training data and making sure that your machine doesn't end up becoming racist or misogynistic as a result of the data that it's trained on. Um, there's even a lot of, uh, interesting thoughts about whether, uh, why and whether, um, uh, intelligent agents should be gendered. Right? So overwhelmingly systems are given female personas, which like perpetuates, um, like female says assistance or subservient somehow. Um, and there's lots of, uh, uh, evidence that people, uh, when they're annoyed engage in pretty.\n\nSummary:", "completion": "There are many ethical considerations when building a dialogue system. This includes making sure the system does not become racist or misogynistic as a result of the training data. The overwhelming majority of systems are given female personas, which can perpetuate stereotypes."}
{"prompt": "Previous summaries:\n\nThe system uses a speech synthesis system that is both realistic and able to give back channel responses.\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\nIn many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology.\nIt is important to consider the ethical issues when building a dialogue system. This includes whether or not to disclose that the system is a machine.\nThere are many ethical considerations when building a dialogue system. This includes making sure the system does not become racist or misogynistic as a result of the training data. The overwhelming majority of systems are given female personas, which can perpetuate stereotypes.\n\nText:\n\nBritish behavior towards their machines. So be kind to your machines, your responses are appearing in chat logs that people analyze. Okay. So just to wrap up, um, there's lots of super interesting research going on now for chat bots. A lot of it is neural network based. Um, there's still lots of, uh, interesting directions that that can be pushed in.\n\nSummary:", "completion": "There is lots of interesting research going on for chat bots. A lot of it is neural network based. There are still lots of interesting directions that that can be pushed in."}
{"prompt": "Previous summaries:\n\nThe system can book a woman's haircut for a client. The system asks for the time and then prepares an answer.\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\nIn many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology.\nIt is important to consider the ethical issues when building a dialogue system. This includes whether or not to disclose that the system is a machine.\nThere are many ethical considerations when building a dialogue system. This includes making sure the system does not become racist or misogynistic as a result of the training data. The overwhelming majority of systems are given female personas, which can perpetuate stereotypes.\nThere is lots of interesting research going on for chat bots. A lot of it is neural network based. There are still lots of interesting directions that that can be pushed in.\n\nText:\n\nUh, the more interesting things in my mind are, how do we use these kind of task oriented dialogue agents, but how do we go beyond just the current state of the art, which is really a small number of handwritten rules with some machine learning on top to kind of come up with deeper methods for understanding and doing, um, state tracking and other sorts of things. All right. So that's it for today's lecture. Um, good luck with your homework assignment and your milestone. Two.\n\nSummary:", "completion": "There are more interesting things to do with task-oriented dialogue agents. The current state of the art is a small number of handwritten rules with some machine learning on top. There are deeper methods for understanding and doing state tracking that can be done."}
{"prompt": "Previous summaries:\n\nThe system can book a woman's haircut for a client. The client's first name is Lisa.\nThe system can make appointments for clients. There are some questions about the ethics of masking as a human to make an appointment.\nThe system can make restaurant reservations for clients. The interaction is seamless because the system has done a good job at eliciting the important things to execute the request.\nThis is another example of the state of the art for these types of systems, which is why the wait time for a table at a restaurant is estimated by asking the user when they would like to eat.\nThere are lots of super cool things that you could do with systems. A lot of research in this has adopted a design methodology where they try to simulate the quality of different elements of that dialogue system using experiments called wizard of Oz experiments.\nIn many studies that intersect with human computer interaction, the methodology is to have people interact with the dialogue system and have someone on the other end listening and providing the responses. This allows for a super cool type of methodology.\nIt is important to consider the ethical issues when building a dialogue system. This includes whether or not to disclose that the system is a machine.\nThere are many ethical considerations when building a dialogue system. This includes making sure the system does not become racist or misogynistic as a result of the training data. The overwhelming majority of systems are given female personas, which can perpetuate stereotypes.\nThere is lots of interesting research going on for chat bots. A lot of it is neural network based. There are still lots of interesting directions that that can be pushed in.\nThere are more interesting things to do with task-oriented dialogue agents. The current state of the art is a small number of handwritten rules with some machine learning on top. There are deeper methods for understanding and doing state tracking that can be done.\n\nText:\n\nwe'll send out notes to the, um, people doing the term project about who your staff, uh, mentor will be on Friday. And if you've got any questions in the meantime, please feel free to reach out to any of the staff members or hop on anyone's office hours to get advice. All I'm gonna hang out for a few minutes. So if anyone's got any questions, feel free to stay on the line. Otherwise I will see you on Monday.\n\nSummary:", "completion": "The term project mentor will be announced on Friday. If you have any questions, feel free to reach out to any staff members or attend office hours."}
